---
title: 'Analysis: henrys'
author: "Paul Hegedus"
output: html_document
---
This page documents the field specific analysis of 'henrys'. This includes analysis for both years of experimentation. Data is split into training and validation datasets using 60 and 40 percent of the data, respectively. For each year, each model is fit to the response variable using backwards AIC based model selection. The final model is used to predict the response variable (yield, protein, net-return) in the validation dataset and for calculating a RMSE for each model. Each variable is then adjusted +/-10% and the response is predicted and compared to the default predictions to determine the most elastic covariates related to each response variable.

For each final model, the AIC and RMSE are recorded in a table with the field, year, AIC, RMSE, and resposne variable. The most sensitive parameters for each model, resposne variable, and year are also recorded. These are used to asses patterns in drivers of the response variable between years in the same field and between fields in the same year, see [ESA Analysis](esa_analysis.html).

```{r}
library(magrittr)
library(ggplot2)
library(dplyr)
source("R/importCleanSelectFuns.R")
source("R/analysisFuns.R")

field_name <- "henrys"
output <- data.frame(field = rep(field_name, 6), year = NA, model = NA, 
                     response = NA, RMSE = NA, AIC = NA)


out_folder <- paste0("results/", field_name," /") 
field_dat <- list.files("prepped")[grepl(field_name, list.files("prepped"))]
```

# 2018 henrys
Analysis for the most recent year on record for the field. The section below imports the data from the specified field and year, explores the relationship between each response and as-applied N, and evaluates the correlations between covariates and responses. Depending on the correlations between parameters, covariates may be omitted. Data is centered if desired, and then 60% of the data is used for training and 40% is used for validation. 

```{r}
year <- "2018"
raw_name <- field_dat[grepl(year, field_dat)]
```

## Import and Explore Data
Import the specified field and year data and summarize the data. See exploratory figures for response variables plotted against each explanatory variable.

```{r}
dat <- impDat(raw_name, "prepped")
summary(dat)
```

Before looking at correlations, parameters with singular gradients are assessed and removed. At this point, the duplicate X and Y columns are removed.

```{r}
zv <- apply(dat, 2, function(x) length(unique(x)) == 1)
dat <- as.data.frame(dat)[, !zv] #(suppose df is the name of your dataset)
dat <- dat[, -grep("X|Y", names(dat))]
```


Now, explore correlations between variables within the dataset to assess any potential covariance issues between variables in analysis. A level of 0.7 is used as a threshold in which to remove a variable due to multicollinearity. 

```{r, fig.height=5, fig.width=7}
cor_mat <- dat %>%
  as.matrix() %>% 
  cor(use = "pairwise.complete.obs") %>%
  round(2)
psych::cor.plot(cor_mat, main=paste0(field_name," Correlation Matrix"), upper=F)
```

Remove any covariates if necessary.

```{r}

```


By default, all of the covariate data except latitude and longitude are centered to decrease the difference in magnitude between the scales of variables (i.e NDVI = 0 - 1, elev = 1500 - 2000). The response variables are also centered so that the variance from the mean is modeled. This allows inference on covariate influence on the variance of the responses around the mean. 

```{r}
dfc <- dat %>% dplyr::select(-c("x", "y"))
for (i in 1:ncol(dfc)) {
  dfc[, i] <- dfc[, i] - mean(dfc[, i], na.rm = T)
}
names(dfc) <- paste0("cent_", names(dfc))
dat <- cbind(dat, dfc)
rm(dfc) # save space in mem
```


The last step before analysis is to split the data into training and validation datasets. 60% of the data is used for training models, while the validation dataset is reserved for assessing the ability of the final model to predict actual values.

```{r}
set.seed(1234)
dat_list <- dualSplit(dat, 0.6)
dt <- dat_list$trn
dv <- dat_list$val
rm(dat_list) # save space in mem
```

## Generalized Additive Model
An initial model is tested that uses a generalized additive model with thin plate shrinkage splines. This combines the model fitting and selection process by allowing the estimated degrees of freedom for coefficients to shrink to zero if that parameter does not have an influence on the response. 

#### Fit Models
Six GAM's for this field are fit, one for each response (yield, protein, and net-return), and using the raw and centered data.
```{r}
modY0 <- mgcv::bam(yld ~ s(aa_n, bs = "ts") + 
                    s(prev_aa_n, bs = "ts") + 
                    s(aspect_rad, bs = "ts") + 
                    s(slope, bs = "ts") +
                    s(elev, bs = "ts") +
                    s(tpi, bs = "ts") +
                    s(prec_py, bs = "ts") +
                    s(gdd_py, k = 1, bs = "ts") + 
                    s(veg_cy, bs = "ts") +
                    s(veg_py, bs = "ts") +
                    s(veg_2py, bs = "ts"),
                  data = dt)
summary(modY0)
```

```{r}
modY0_cent <- mgcv::bam(yld ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_prev_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modY0_cent)
```

```{r}
modP0 <- mgcv::bam(pro ~ s(aa_n, bs = "ts") + 
                    s(prev_aa_n, bs = "ts") + 
                    s(aspect_rad, bs = "ts") + 
                    s(slope, bs = "ts") +
                    s(elev, bs = "ts") +
                    s(tpi, bs = "ts") +
                    s(prec_py, bs = "ts") +
                    s(gdd_py, k = 1, bs = "ts") + 
                    s(veg_cy, bs = "ts") +
                    s(veg_py, bs = "ts") +
                    s(veg_2py, bs = "ts"),
                  data = dt)
summary(modP0)
```

```{r}
modP0_cent <- mgcv::bam(pro ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_prev_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modP0_cent)
```

```{r}
modNR0 <- mgcv::bam(NR ~ s(aa_n, bs = "ts") + 
                    s(prev_aa_n, bs = "ts") + 
                    s(aspect_rad, bs = "ts") + 
                    s(slope, bs = "ts") +
                    s(elev, bs = "ts") +
                    s(tpi, bs = "ts") +
                    s(prec_py, bs = "ts") +
                    s(gdd_py, k = 1, bs = "ts") + 
                    s(veg_cy, bs = "ts") +
                    s(veg_py, bs = "ts") +
                    s(veg_2py, bs = "ts"),
                  data = dt)
summary(modNR0)
```

```{r}
modNR0_cent <- mgcv::bam(NR ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_prev_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modNR0_cent)
```

#### Check Diagnostics
For each model fit, the diagnostics are checked to assess the assumptions of each model. These include linearity, normality of residuals, homogeneity of variance, and independence.

First make a directory to save outputs.
```{r}
cwd <- paste0("results/", field_name, "/diagnostics")
if (!file.exists(cwd)) {
  dir.create(cwd)
}
```

Look at yield models.
```{r}
par(mfrow=c(2,2))
png(paste0(cwd, "/yld_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modY0)
dev.off()
mgcv::gam.check(modY0)

png(paste0(cwd, "/yld_cent_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modY0_cent)
dev.off()
mgcv::gam.check(modY0_cent)
```

Look at protein models.
```{r} 
par(mfrow=c(2,2))
png(paste0(cwd, "/pro_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modP0)
dev.off()
mgcv::gam.check(modP0)

png(paste0(cwd, "/pro_cent_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modP0_cent)
dev.off()
mgcv::gam.check(modP0_cent)
```

Look at net-return models.
```{r}
par(mfrow=c(2,2))
png(paste0(cwd, "/NR_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modNR0)
dev.off()
mgcv::gam.check(modNR0)

png(paste0(cwd, "/NR_cent_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)  
  par(mfrow=c(2,2))
  mgcv::gam.check(modNR0_cent)
dev.off()
mgcv::gam.check(modNR0_cent)
```

#### Update Models
Although there seems to be some heavy left tails in the normal qq-plots and violations of homogeneity of variance, we remind ourselves that the ultimate goal of these models is to make the best prediction, not necessarily the best interpretations. However, we are going to address the violation of the assumption of independence, because data closer to each other are more related than data farther apart from each other.

To take spatial autocorrelation into account, a gaussian process basis prediction function is used on the product of latitude and longitude.

```{r}
modY1 <- mgcv::bam(yld ~ s(x, y, bs = "gp", m = 2) +
                    s(aa_n, bs = "ts") + 
                    s(prev_aa_n, bs = "ts") + 
                    s(aspect_rad, bs = "ts") + 
                    s(slope, bs = "ts") +
                    s(elev, bs = "ts") +
                    s(tpi, bs = "ts") +
                    s(prec_py, bs = "ts") +
                    s(gdd_py, k = 1, bs = "ts") + 
                    s(veg_cy, bs = "ts") +
                    s(veg_py, bs = "ts") +
                    s(veg_2py, bs = "ts"),
                  data = dt)
modY1_cent <- mgcv::bam(yld ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_prev_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
modP1 <- mgcv::bam(pro ~ s(x, y, bs = "gp", m = 2) +
                    s(aa_n, bs = "ts") + 
                    s(prev_aa_n, bs = "ts") + 
                    s(aspect_rad, bs = "ts") + 
                    s(slope, bs = "ts") +
                    s(elev, bs = "ts") +
                    s(tpi, bs = "ts") +
                    s(prec_py, bs = "ts") +
                    s(gdd_py, k = 1, bs = "ts") + 
                    s(veg_cy, bs = "ts") +
                    s(veg_py, bs = "ts") +
                    s(veg_2py, bs = "ts"),
                  data = dt)
modP1_cent <- mgcv::bam(pro ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_prev_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
modNR1 <- mgcv::bam(NR ~ s(x, y, bs = "gp", m = 2) +
                    s(aa_n, bs = "ts") + 
                    s(prev_aa_n, bs = "ts") + 
                    s(aspect_rad, bs = "ts") + 
                    s(slope, bs = "ts") +
                    s(elev, bs = "ts") +
                    s(tpi, bs = "ts") +
                    s(prec_py, bs = "ts") +
                    s(gdd_py, k = 1, bs = "ts") + 
                    s(veg_cy, bs = "ts") +
                    s(veg_py, bs = "ts") +
                    s(veg_2py, bs = "ts"),
                  data = dt)
modNR1_cent <- mgcv::bam(NR ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_prev_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
```
#### Assess Models
To asses the predictive ability of each model, it is used to predict the response in the validation dataset. This allows us to calculate RMSE and make plots of the model fits compared to observed values.

```{r}
dv$gam_pred_yld <- predict(modY1, dv)
dv$gam_pred_yld_cent <- predict(modY1_cent, dv)
dv$gam_pred_pro <- predict(modP1, dv)
dv$gam_pred_pro_cent <- predict(modP1_cent, dv)
dv$gam_pred_NR <- predict(modNR1, dv)
dv$gam_pred_NR_cent <- predict(modNR1, dv)

plotVal(dv, "yld", "gam_pred_yld", field_name, year, modY1, TRUE)
plotVal(dv, "yld", "gam_pred_yld_cent", field_name, year, modY1_cent, TRUE)
plotVal(dv, "pro", "gam_pred_pro", field_name, year, modP1, TRUE)
plotVal(dv, "pro", "gam_pred_pro_cent", field_name, year, modP1_cent, TRUE)
plotVal(dv, "NR", "gam_pred_NR", field_name, year, modNR1, TRUE)
plotVal(dv, "NR", "gam_pred_NR_cent", field_name, year, modNR1_cent, TRUE)
```

Based on the fitted values, there does not appear to be any difference between models fit with raw or centered response variables, so raw values will be assessed. Now, plot the observed vs. predcited values and calculate RMSE.

```{r}
plotObsVPreds(dv, "yld", "gam_pred_yld", field_name, year, TRUE)
plotObsVPreds(dv, "yld", "gam_pred_yld_cent", field_name, year, TRUE)
plotObsVPreds(dv, "pro", "gam_pred_pro", field_name, year, TRUE)
plotObsVPreds(dv, "pro", "gam_pred_pro_cent", field_name, year, TRUE)
plotObsVPreds(dv, "NR", "gam_pred_NR", field_name, year, TRUE)
plotObsVPreds(dv, "NR", "gam_pred_NR_cent", field_name, year, TRUE)
```

#### Save Outputs
Now that each model has been fit, the RMSE and AIC of each is recorded for further analysis and comparison between years and across years. Again, because centering the responses results in the exact same model fits, it is omitted.

```{r}
rmses <- c(round(Metrics::rmse(na.omit(dv$yld), na.omit(dv$gam_pred_yld)), 4),
           round(Metrics::rmse(na.omit(dv$pro), na.omit(dv$gam_pred_pro)), 4),
           round(Metrics::rmse(na.omit(dv$NR), na.omit(dv$gam_pred_NR)), 4))
aics <- c(AIC(modY0), AIC(modP0), AIC(modNR0))
forOutput <- data.frame(rep(field_name, 3), rep(year, 3), rep("GAM", 3),
                        c("Yield", "Protein", "Net-Return"), rmses, aics)
# EDIT
output[1:3, ] <- forOutput
```

#### Sensitivity Analysis
To perform the sensitivity analysis, each variable is increased or decreased by 10%. The resulting mean prediction across the field for the response is recorded and compared against the mean responses without adjustment. Sensitivity is calculated as the absolute value of the difference between the mean response of the default minus the mean response in the adjusted setting, over the mean response in the default system, divided by 0.1. 

Make tables to hold sensitivity analysis results.

```{r}
covars <- c("aa_n", "aspect_rad", "slope", "elev",
            "tpi", "prec_py", "gdd_py", "veg_cy",
            "veg_py", "veg_2py")

yldSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(yldSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
yldSensTab$ResponseVar <- "yld"
yldSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
yldSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))

proSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(proSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
proSensTab$ResponseVar <- "pro"
proSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
proSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))


NRSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(NRSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
NRSensTab$ResponseVar <- "NR"
NRSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
NRSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))
```

Run sensitivity analysis.
```{r}
yldSensTab <- sensitivity(yldSensTab, "yld", dv, modY1)
proSensTab <- sensitivity(proSensTab, "pro", dv, modP1)
NRSensTab <- sensitivity(NRSensTab, "NR", dv, modNR1)
```

Repeat for centered data.
```{r}
covars <- c("cent_aa_n", "cent_aspect_rad", "cent_slope", "cent_elev",
            "cent_tpi", "cent_prec_py", "cent_gdd_py", "cent_veg_cy",
            "cent_veg_py", "cent_veg_2py")

centyldSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(centyldSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
centyldSensTab$ResponseVar <- "yld"
centyldSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
centyldSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))

centproSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(centproSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
centproSensTab$ResponseVar <- "pro"
centproSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
centproSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))


centNRSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(centNRSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
centNRSensTab$ResponseVar <- "NR"
centNRSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
centNRSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))
```

Run sensitivity analysis.
```{r}
centyldSensTab <- sensitivity(centyldSensTab, "yld", dv, modY1_cent)
centproSensTab <- sensitivity(centproSensTab, "pro", dv, modP1_cent)
centNRSensTab <- sensitivity(centNRSensTab, "NR", dv, modNR1_cent)
```


#### Save Outputs
```{r}
sensTable <- rbind(yldSensTab, proSensTab, NRSensTab)
sensTable$field <- field_name
data.table::fwrite(sensTable,
                   paste0("results/", field_name, "/", 
                          field_name, "_sensTable_", year))
sensTable <- rbind(centyldSensTab, centproSensTab, centNRSensTab)
sensTable$field <- field_name
data.table::fwrite(sensTable,
                   paste0("results/", field_name, "/", 
                          field_name, "_sensTableCent_", year))
```

# 2016 henrys
Analysis for the most recent year on record for the field. The section below imports the data from the specified field and year, explores the relationship between each response and as-applied N, and evaluates the correlations between covariates and responses. Depending on the correlations between parameters, covariates may be omitted. Data is centered if desired, and then 60% of the data is used for training and 40% is used for validation. 

```{r}
year <- "2016"
raw_name <- field_dat[grepl(year, field_dat)]
```

## Import and Explore Data
Import the specified field and year data and summarize the data. See exploratory figures for response variables plotted against each explanatory variable.

```{r}
dat <- impDat(raw_name, "prepped")
summary(dat)
```

Before looking at correlations, parameters with singular gradients are assessed and removed. At this point, the duplicate X and Y columns are removed.

```{r}
zv <- apply(dat, 2, function(x) length(unique(x)) == 1)
dat <- as.data.frame(dat)[, !zv] #(suppose df is the name of your dataset)
dat <- dat[, -grep("X|Y", names(dat))]
```


Now, explore correlations between variables within the dataset to assess any potential covariance issues between variables in analysis. A level of 0.7 is used as a threshold in which to remove a variable due to multicollinearity. 

```{r, fig.height=5, fig.width=7}
cor_mat <- dat %>%
  as.matrix() %>% 
  cor(use = "pairwise.complete.obs") %>%
  round(2)
psych::cor.plot(cor_mat, main=paste0(field_name," Correlation Matrix"), upper=F)
```

Remove any covariates if necessary.

```{r}

```


By default, all of the covariate data except latitude and longitude are centered to decrease the difference in magnitude between the scales of variables (i.e NDVI = 0 - 1, elev = 1500 - 2000). The response variables are also centered so that the variance from the mean is modeled. This allows inference on covariate influence on the variance of the responses around the mean. 

```{r}
dfc <- dat %>% dplyr::select(-c("x", "y"))
for (i in 1:ncol(dfc)) {
  dfc[, i] <- dfc[, i] - mean(dfc[, i], na.rm = T)
}
names(dfc) <- paste0("cent_", names(dfc))
dat <- cbind(dat, dfc)
rm(dfc) # save space in mem
```


The last step before analysis is to split the data into training and validation datasets. 60% of the data is used for training models, while the validation dataset is reserved for assessing the ability of the final model to predict actual values.

```{r}
dat_list <- dualSplit(dat, 0.6)
dt <- dat_list$trn
dv <- dat_list$val
rm(dat_list) # save space in mem
```

## Generalized Additive Model
An initial model is tested that uses a generalized additive model with thin plate shrinkage splines. This combines the model fitting and selection process by allowing the estimated degrees of freedom for coefficients to shrink to zero if that parameter does not have an influence on the response. 

#### Fit Models
Six GAM's for this field are fit, one for each response (yield, protein, and net-return), and using the raw and centered data.
```{r}
modY0 <- mgcv::bam(yld ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modY0)
```

```{r}
modY0_cent <- mgcv::bam(yld ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modY0_cent)
```

```{r}
modP0 <- mgcv::bam(pro ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modP0)
```

```{r}
modP0_cent <- mgcv::bam(pro ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modP0_cent)
```

```{r}
modNR0 <- mgcv::bam(NR ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modNR0)
```

```{r}
modNR0_cent <- mgcv::bam(NR ~ s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
summary(modNR0_cent)
```

#### Check Diagnostics
For each model fit, the diagnostics are checked to assess the assumptions of each model. These include linearity, normality of residuals, homogeneity of variance, and independence.

First make a directory to save outputs.
```{r}
cwd <- paste0("results/", field_name, "/diagnostics")
if (!file.exists(cwd)) {
  dir.create(cwd)
}
```

Look at yield models.
```{r}
par(mfrow=c(2,2))
png(paste0(cwd, "/yld_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modY0)
dev.off()
mgcv::gam.check(modY0)

png(paste0(cwd, "/yld_cent_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modY0_cent)
dev.off()
mgcv::gam.check(modY0_cent)
```

Look at protein models.
```{r} 
par(mfrow=c(2,2))
png(paste0(cwd, "/pro_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modP0)
dev.off()
mgcv::gam.check(modP0)

png(paste0(cwd, "/pro_cent_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modP0_cent)
dev.off()
mgcv::gam.check(modP0_cent)
```

Look at net-return models.
```{r}
par(mfrow=c(2,2))
png(paste0(cwd, "/NR_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)
  par(mfrow=c(2,2))
  mgcv::gam.check(modNR0)
dev.off()
mgcv::gam.check(modNR0)

png(paste0(cwd, "/NR_cent_diagnostics.png"), 
    width = 10, 
    height = 10,
    units = 'in', 
    res = 100)  
  par(mfrow=c(2,2))
  mgcv::gam.check(modNR0_cent)
dev.off()
mgcv::gam.check(modNR0_cent)
```

#### Update Models
Although there seems to be some heavy left tails in the normal qq-plots and violations of homogeneity of variance, we remind ourselves that the ultimate goal of these models is to make the best prediction, not necessarily the best interpretations. However, we are going to address the violation of the assumption of independence, because data closer to each other are more related than data farther apart from each other.

To take spatial autocorrelation into account, a gaussian process basis prediction function is used on the product of latitude and longitude.

```{r}
modY1 <- mgcv::bam(yld ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
modY1_cent <- mgcv::bam(yld ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
modP1 <- mgcv::bam(pro ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
modP1_cent <- mgcv::bam(pro ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
modNR1 <- mgcv::bam(NR ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
modNR1_cent <- mgcv::bam(NR ~ s(x, y, bs = "gp", m = 2) +
                    s(cent_aa_n, bs = "ts") + 
                    s(cent_aspect_rad, bs = "ts") + 
                    s(cent_slope, bs = "ts") +
                    s(cent_elev, bs = "ts") +
                    s(cent_tpi, bs = "ts") +
                    s(cent_prec_py, bs = "ts") +
                    s(cent_gdd_py, k = 1, bs = "ts") + 
                    s(cent_veg_cy, bs = "ts") +
                    s(cent_veg_py, bs = "ts") +
                    s(cent_veg_2py, bs = "ts"),
                  data = dt)
```
#### Assess Models
To asses the predictive ability of each model, it is used to predict the response in the validation dataset. This allows us to calculate RMSE and make plots of the model fits compared to observed values.

```{r}
dv$gam_pred_yld <- predict(modY1, dv)
dv$gam_pred_yld_cent <- predict(modY1_cent, dv)
dv$gam_pred_pro <- predict(modP1, dv)
dv$gam_pred_pro_cent <- predict(modP1_cent, dv)
dv$gam_pred_NR <- predict(modNR1, dv)
dv$gam_pred_NR_cent <- predict(modNR1, dv)

plotVal(dv, "yld", "gam_pred_yld", field_name, year, modY1, TRUE)
plotVal(dv, "yld", "gam_pred_yld_cent", field_name, year, modY1_cent, TRUE)
plotVal(dv, "pro", "gam_pred_pro", field_name, year, modP1, TRUE)
plotVal(dv, "pro", "gam_pred_pro_cent", field_name, year, modP1_cent, TRUE)
plotVal(dv, "NR", "gam_pred_NR", field_name, year, modNR1, TRUE)
plotVal(dv, "NR", "gam_pred_NR_cent", field_name, year, modNR1_cent, TRUE)
```

Based on the fitted values, there does not appear to be any difference between models fit with raw or centered response variables, so raw values will be assessed. Now, plot the observed vs. predcited values and calculate RMSE.

```{r}
plotObsVPreds(dv, "yld", "gam_pred_yld", field_name, year, TRUE)
plotObsVPreds(dv, "yld", "gam_pred_yld_cent", field_name, year, TRUE)
plotObsVPreds(dv, "pro", "gam_pred_pro", field_name, year, TRUE)
plotObsVPreds(dv, "pro", "gam_pred_pro_cent", field_name, year, TRUE)
plotObsVPreds(dv, "NR", "gam_pred_NR", field_name, year, TRUE)
plotObsVPreds(dv, "NR", "gam_pred_NR_cent", field_name, year, TRUE)
```

#### Save Outputs
Now that each model has been fit, the RMSE and AIC of each is recorded for further analysis and comparison between years and across years. Again, because centering the responses results in the exact same model fits, it is omitted.

```{r}
rmses <- c(round(Metrics::rmse(na.omit(dv$yld), na.omit(dv$gam_pred_yld)), 4),
           round(Metrics::rmse(na.omit(dv$pro), na.omit(dv$gam_pred_pro)), 4),
           round(Metrics::rmse(na.omit(dv$NR), na.omit(dv$gam_pred_NR)), 4))
aics <- c(AIC(modY0), AIC(modP0), AIC(modNR0))
forOutput <- data.frame(rep(field_name, 3), rep(year, 3), rep("GAM", 3),
                        c("Yield", "Protein", "Net-Return"), rmses, aics)
# EDIT
output[4:6, ] <- forOutput
```

#### Sensitivity Analysis
To perform the sensitivity analysis, each variable is increased or decreased by 10%. The resulting mean prediction across the field for the response is recorded and compared against the mean responses without adjustment. Sensitivity is calculated as the absolute value of the difference between the mean response of the default minus the mean response in the adjusted setting, over the mean response in the default system, divided by 0.1. 

Make tables to hold sensitivity analysis results.

```{r}
covars <- c("aa_n", "aspect_rad", "slope", "elev",
            "tpi", "prec_py", "gdd_py", "veg_cy",
            "veg_py", "veg_2py")

yldSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(yldSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
yldSensTab$ResponseVar <- "yld"
yldSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
yldSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))

proSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(proSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
proSensTab$ResponseVar <- "pro"
proSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
proSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))


NRSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(NRSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
NRSensTab$ResponseVar <- "NR"
NRSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
NRSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))
```

Run sensitivity analysis.
```{r}
yldSensTab <- sensitivity(yldSensTab, "yld", dv, modY1)
proSensTab <- sensitivity(proSensTab, "pro", dv, modP1)
NRSensTab <- sensitivity(NRSensTab, "NR", dv, modNR1)
```

Repeat for centered data.
```{r}
covars <- c("cent_aa_n", "cent_aspect_rad", "cent_slope", "cent_elev",
            "cent_tpi", "cent_prec_py", "cent_gdd_py", "cent_veg_cy",
            "cent_veg_py", "cent_veg_2py")

centyldSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(centyldSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
centyldSensTab$ResponseVar <- "yld"
centyldSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
centyldSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))

centproSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(centproSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
centproSensTab$ResponseVar <- "pro"
centproSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
centproSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))


centNRSensTab <- matrix(0, nrow = length(covars)*2 + 1, ncol = 5) %>%
  as.data.frame()
names(centNRSensTab) <- c("Parameter", "Adjustment", "MeanResp",
                       "Sensitivity.value","Elasticity.value")
centNRSensTab$ResponseVar <- "NR"
centNRSensTab$Parameter <- c("Defaults", rep(covars, each = 2))
centNRSensTab$Adjustment <- c(NA, rep(c("10", "-10"), length(covars)))
```

Run sensitivity analysis.
```{r}
centyldSensTab <- sensitivity(centyldSensTab, "yld", dv, modY1_cent)
centproSensTab <- sensitivity(centproSensTab, "pro", dv, modP1_cent)
centNRSensTab <- sensitivity(centNRSensTab, "NR", dv, modNR1_cent)
```


#### Save Outputs
```{r}
sensTable <- rbind(yldSensTab, proSensTab, NRSensTab)
sensTable$field <- field_name
data.table::fwrite(sensTable,
                   paste0("results/", field_name, "/", 
                          field_name, "_sensTable_", year))
sensTable <- rbind(centyldSensTab, centproSensTab, centNRSensTab)
sensTable$field <- field_name
data.table::fwrite(sensTable,
                   paste0("results/", field_name, "/", 
                          field_name, "_sensTableCent_", year))
```